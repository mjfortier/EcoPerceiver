{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c37e6694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from ecoperceiver.dataset import EcoPerceiverLoaderConfig, EcoPerceiverDataset\n",
    "from ecoperceiver.components import EcoPerceiverConfig\n",
    "from ecoperceiver.model import EcoPerceiver\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b78d0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.yml  misc.py\t run_experiment.py  submit_CC.py  testbench.ipynb\n",
      "data\t    __pycache__  runs\t\t    tensorboard\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6842fc86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing sites...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.20it/s]\n"
     ]
    }
   ],
   "source": [
    "db_path = Path('data/carbonsense_v2')\n",
    "ds = EcoPerceiverDataset(db_path, config=EcoPerceiverLoaderConfig(targets=['NEE']), sites=['CA-Gro'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cd0463",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /home/l/luislara/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 75.8MB/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m config = EcoPerceiverConfig(targets=(\u001b[33m'\u001b[39m\u001b[33mNEE\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mGPP\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mFCH4\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m      2\u001b[39m model = EcoPerceiver(config)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mmodel loaded\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/l/luislara/env/ecoperceiver/lib/python3.12/site-packages/torch/nn/modules/module.py:1343\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1340\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1341\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/l/luislara/env/ecoperceiver/lib/python3.12/site-packages/torch/nn/modules/module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/l/luislara/env/ecoperceiver/lib/python3.12/site-packages/torch/nn/modules/module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/l/luislara/env/ecoperceiver/lib/python3.12/site-packages/torch/nn/modules/module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/l/luislara/env/ecoperceiver/lib/python3.12/site-packages/torch/nn/modules/module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    927\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    928\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    929\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    931\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    933\u001b[39m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/l/luislara/env/ecoperceiver/lib/python3.12/site-packages/torch/nn/modules/module.py:1329\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1322\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1323\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1324\u001b[39m             device,\n\u001b[32m   1325\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1326\u001b[39m             non_blocking,\n\u001b[32m   1327\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1328\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1329\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1335\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/l/luislara/env/ecoperceiver/lib/python3.12/site-packages/torch/cuda/__init__.py:319\u001b[39m, in \u001b[36m_lazy_init\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mCUDA_MODULE_LOADING\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os.environ:\n\u001b[32m    318\u001b[39m     os.environ[\u001b[33m\"\u001b[39m\u001b[33mCUDA_MODULE_LOADING\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mLAZY\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[32m    321\u001b[39m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[32m    322\u001b[39m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[32m    323\u001b[39m _tls.is_initializing = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "# config = EcoPerceiverConfig(targets=('NEE', 'GPP', 'FCH4'))\n",
    "# model = EcoPerceiver(config)\n",
    "model.to('cuda')\n",
    "print('model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e32f08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(ds, batch_size=64, shuffle=True, num_workers=8, pin_memory=True, collate_fn=ds.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07ec9f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acfcd337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 32, 2])\n"
     ]
    }
   ],
   "source": [
    "print(batch.target_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ed07c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting NEE\n",
      "torch.Size([64])\n",
      "Computing loss...\n",
      "1\n",
      "torch.Size([64])\n",
      "tensor([  0.1756,  -1.3085,   1.2390,   2.6056,  -3.2233,  -1.0509,   0.7561,\n",
      "         -2.0395,   1.1330,   4.3896,  -2.9610, -17.3384,   1.6196,  -4.7775,\n",
      "          0.4212,  -1.5463,   8.7870,   0.4910,   7.8065,   1.1089,   1.0605,\n",
      "          7.0232,   2.7902,   0.7532,   0.0432,  -1.0631,  -7.6898,   3.2061,\n",
      "          0.3035,   0.0781,   0.7475,   0.4586,   0.5450,  -4.0103,   0.0185,\n",
      "          1.0924,   0.8335,   1.4085,   0.5809,  -9.7771,   5.8708,   4.2325,\n",
      "          5.8984,   0.8907,   5.8781,  -1.0245,   1.4685,  14.3005,  -5.1600,\n",
      "          5.5245,   6.3771,  -6.6098, -12.0870,   0.2754,  -3.4214,   4.8960,\n",
      "          3.3321,   0.1624,   4.8995,  -9.2638, -10.0200,   0.6183,   0.8913,\n",
      "          0.3577], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "op = model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4106030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EcoPerceiverOutput(flux_labels=('NEE',), predictions=tensor([[11.8248],\n",
       "        [ 6.8123],\n",
       "        [ 6.0864],\n",
       "        [ 9.7126],\n",
       "        [ 9.8189],\n",
       "        [16.1592],\n",
       "        [ 8.6755],\n",
       "        [18.4786],\n",
       "        [ 8.4964],\n",
       "        [ 2.5009],\n",
       "        [10.7414],\n",
       "        [ 9.8551],\n",
       "        [ 9.3382],\n",
       "        [10.4671],\n",
       "        [ 9.4199],\n",
       "        [11.8050],\n",
       "        [11.2545],\n",
       "        [ 7.6609],\n",
       "        [ 4.1920],\n",
       "        [11.0148],\n",
       "        [ 0.3522],\n",
       "        [11.2071],\n",
       "        [ 8.5418],\n",
       "        [ 8.5092],\n",
       "        [ 5.9465],\n",
       "        [13.4684],\n",
       "        [10.4559],\n",
       "        [11.1878],\n",
       "        [ 0.0259],\n",
       "        [ 7.8133],\n",
       "        [ 3.2237],\n",
       "        [13.6359],\n",
       "        [12.5265],\n",
       "        [12.5869],\n",
       "        [ 9.2152],\n",
       "        [ 6.0716],\n",
       "        [ 5.8113],\n",
       "        [ 8.9191],\n",
       "        [-2.9655],\n",
       "        [ 2.0849],\n",
       "        [ 7.0496],\n",
       "        [ 7.7219],\n",
       "        [ 2.9040],\n",
       "        [ 9.4338],\n",
       "        [-3.0099],\n",
       "        [ 8.0190],\n",
       "        [ 6.7518],\n",
       "        [ 8.6909],\n",
       "        [12.1057],\n",
       "        [ 7.5305],\n",
       "        [ 7.0699],\n",
       "        [ 6.4164],\n",
       "        [10.4783],\n",
       "        [14.9306],\n",
       "        [ 6.0061],\n",
       "        [12.5150],\n",
       "        [ 6.0743],\n",
       "        [ 2.2384],\n",
       "        [ 8.9294],\n",
       "        [11.4278],\n",
       "        [12.5105],\n",
       "        [14.9935],\n",
       "        [ 9.7743],\n",
       "        [ 4.0855]], device='cuda:0', grad_fn=<StackBackward0>), ground_truth=tensor([[  0.1756],\n",
       "        [ -1.3085],\n",
       "        [  1.2390],\n",
       "        [  2.6056],\n",
       "        [ -3.2233],\n",
       "        [ -1.0509],\n",
       "        [  0.7561],\n",
       "        [ -2.0395],\n",
       "        [  1.1330],\n",
       "        [  4.3896],\n",
       "        [ -2.9610],\n",
       "        [-17.3384],\n",
       "        [  1.6196],\n",
       "        [ -4.7775],\n",
       "        [  0.4212],\n",
       "        [ -1.5463],\n",
       "        [  8.7870],\n",
       "        [  0.4910],\n",
       "        [  7.8065],\n",
       "        [  1.1089],\n",
       "        [  1.0605],\n",
       "        [  7.0232],\n",
       "        [  2.7902],\n",
       "        [  0.7532],\n",
       "        [  0.0432],\n",
       "        [ -1.0631],\n",
       "        [ -7.6898],\n",
       "        [  3.2061],\n",
       "        [  0.3035],\n",
       "        [  0.0781],\n",
       "        [  0.7475],\n",
       "        [  0.4586],\n",
       "        [  0.5450],\n",
       "        [ -4.0103],\n",
       "        [  0.0185],\n",
       "        [  1.0924],\n",
       "        [  0.8335],\n",
       "        [  1.4085],\n",
       "        [  0.5809],\n",
       "        [ -9.7771],\n",
       "        [  5.8708],\n",
       "        [  4.2325],\n",
       "        [  5.8984],\n",
       "        [  0.8907],\n",
       "        [  5.8781],\n",
       "        [ -1.0245],\n",
       "        [  1.4685],\n",
       "        [ 14.3005],\n",
       "        [ -5.1600],\n",
       "        [  5.5245],\n",
       "        [  6.3771],\n",
       "        [ -6.6098],\n",
       "        [-12.0870],\n",
       "        [  0.2754],\n",
       "        [ -3.4214],\n",
       "        [  4.8960],\n",
       "        [  3.3321],\n",
       "        [  0.1624],\n",
       "        [  4.8995],\n",
       "        [ -9.2638],\n",
       "        [-10.0200],\n",
       "        [  0.6183],\n",
       "        [  0.8913],\n",
       "        [  0.3577]], device='cuda:0'), loss=tensor([118.8360], device='cuda:0', grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecoperceiver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
