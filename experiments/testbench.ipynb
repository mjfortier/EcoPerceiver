{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c37e6694",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matt/miniconda3/envs/scratch/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/matt/miniconda3/envs/scratch/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c104cuda9SetDeviceEi'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/home/matt/miniconda3/envs/scratch/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from ecoperceiver.dataset import EcoPerceiverLoaderConfig, EcoPerceiverDataset\n",
    "from ecoperceiver.components import EcoPerceiverConfig\n",
    "from ecoperceiver.model import EcoPerceiver\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6842fc86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing sites...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 41.43it/s]\n"
     ]
    }
   ],
   "source": [
    "db_path = Path('/data/fluxes/carbonsense_v2')\n",
    "ds = EcoPerceiverDataset(db_path, config=EcoPerceiverLoaderConfig(targets=['FCH4', 'NEE']), sites=['CA-Gro', 'US-Wi3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cd0463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n"
     ]
    }
   ],
   "source": [
    "config = EcoPerceiverConfig(targets=('NEE', 'GPP', 'FCH4'))\n",
    "model = EcoPerceiver(config)\n",
    "model.to('cuda')\n",
    "print('model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e32f08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(ds, batch_size=64, shuffle=True, num_workers=8, pin_memory=True, collate_fn=ds.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07ec9f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acfcd337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 32, 2])\n"
     ]
    }
   ],
   "source": [
    "print(batch.target_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ed07c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting NEE\n",
      "torch.Size([64])\n",
      "Computing loss...\n",
      "1\n",
      "torch.Size([64])\n",
      "tensor([  0.1756,  -1.3085,   1.2390,   2.6056,  -3.2233,  -1.0509,   0.7561,\n",
      "         -2.0395,   1.1330,   4.3896,  -2.9610, -17.3384,   1.6196,  -4.7775,\n",
      "          0.4212,  -1.5463,   8.7870,   0.4910,   7.8065,   1.1089,   1.0605,\n",
      "          7.0232,   2.7902,   0.7532,   0.0432,  -1.0631,  -7.6898,   3.2061,\n",
      "          0.3035,   0.0781,   0.7475,   0.4586,   0.5450,  -4.0103,   0.0185,\n",
      "          1.0924,   0.8335,   1.4085,   0.5809,  -9.7771,   5.8708,   4.2325,\n",
      "          5.8984,   0.8907,   5.8781,  -1.0245,   1.4685,  14.3005,  -5.1600,\n",
      "          5.5245,   6.3771,  -6.6098, -12.0870,   0.2754,  -3.4214,   4.8960,\n",
      "          3.3321,   0.1624,   4.8995,  -9.2638, -10.0200,   0.6183,   0.8913,\n",
      "          0.3577], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "op = model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4106030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EcoPerceiverOutput(flux_labels=('NEE',), predictions=tensor([[11.8248],\n",
       "        [ 6.8123],\n",
       "        [ 6.0864],\n",
       "        [ 9.7126],\n",
       "        [ 9.8189],\n",
       "        [16.1592],\n",
       "        [ 8.6755],\n",
       "        [18.4786],\n",
       "        [ 8.4964],\n",
       "        [ 2.5009],\n",
       "        [10.7414],\n",
       "        [ 9.8551],\n",
       "        [ 9.3382],\n",
       "        [10.4671],\n",
       "        [ 9.4199],\n",
       "        [11.8050],\n",
       "        [11.2545],\n",
       "        [ 7.6609],\n",
       "        [ 4.1920],\n",
       "        [11.0148],\n",
       "        [ 0.3522],\n",
       "        [11.2071],\n",
       "        [ 8.5418],\n",
       "        [ 8.5092],\n",
       "        [ 5.9465],\n",
       "        [13.4684],\n",
       "        [10.4559],\n",
       "        [11.1878],\n",
       "        [ 0.0259],\n",
       "        [ 7.8133],\n",
       "        [ 3.2237],\n",
       "        [13.6359],\n",
       "        [12.5265],\n",
       "        [12.5869],\n",
       "        [ 9.2152],\n",
       "        [ 6.0716],\n",
       "        [ 5.8113],\n",
       "        [ 8.9191],\n",
       "        [-2.9655],\n",
       "        [ 2.0849],\n",
       "        [ 7.0496],\n",
       "        [ 7.7219],\n",
       "        [ 2.9040],\n",
       "        [ 9.4338],\n",
       "        [-3.0099],\n",
       "        [ 8.0190],\n",
       "        [ 6.7518],\n",
       "        [ 8.6909],\n",
       "        [12.1057],\n",
       "        [ 7.5305],\n",
       "        [ 7.0699],\n",
       "        [ 6.4164],\n",
       "        [10.4783],\n",
       "        [14.9306],\n",
       "        [ 6.0061],\n",
       "        [12.5150],\n",
       "        [ 6.0743],\n",
       "        [ 2.2384],\n",
       "        [ 8.9294],\n",
       "        [11.4278],\n",
       "        [12.5105],\n",
       "        [14.9935],\n",
       "        [ 9.7743],\n",
       "        [ 4.0855]], device='cuda:0', grad_fn=<StackBackward0>), ground_truth=tensor([[  0.1756],\n",
       "        [ -1.3085],\n",
       "        [  1.2390],\n",
       "        [  2.6056],\n",
       "        [ -3.2233],\n",
       "        [ -1.0509],\n",
       "        [  0.7561],\n",
       "        [ -2.0395],\n",
       "        [  1.1330],\n",
       "        [  4.3896],\n",
       "        [ -2.9610],\n",
       "        [-17.3384],\n",
       "        [  1.6196],\n",
       "        [ -4.7775],\n",
       "        [  0.4212],\n",
       "        [ -1.5463],\n",
       "        [  8.7870],\n",
       "        [  0.4910],\n",
       "        [  7.8065],\n",
       "        [  1.1089],\n",
       "        [  1.0605],\n",
       "        [  7.0232],\n",
       "        [  2.7902],\n",
       "        [  0.7532],\n",
       "        [  0.0432],\n",
       "        [ -1.0631],\n",
       "        [ -7.6898],\n",
       "        [  3.2061],\n",
       "        [  0.3035],\n",
       "        [  0.0781],\n",
       "        [  0.7475],\n",
       "        [  0.4586],\n",
       "        [  0.5450],\n",
       "        [ -4.0103],\n",
       "        [  0.0185],\n",
       "        [  1.0924],\n",
       "        [  0.8335],\n",
       "        [  1.4085],\n",
       "        [  0.5809],\n",
       "        [ -9.7771],\n",
       "        [  5.8708],\n",
       "        [  4.2325],\n",
       "        [  5.8984],\n",
       "        [  0.8907],\n",
       "        [  5.8781],\n",
       "        [ -1.0245],\n",
       "        [  1.4685],\n",
       "        [ 14.3005],\n",
       "        [ -5.1600],\n",
       "        [  5.5245],\n",
       "        [  6.3771],\n",
       "        [ -6.6098],\n",
       "        [-12.0870],\n",
       "        [  0.2754],\n",
       "        [ -3.4214],\n",
       "        [  4.8960],\n",
       "        [  3.3321],\n",
       "        [  0.1624],\n",
       "        [  4.8995],\n",
       "        [ -9.2638],\n",
       "        [-10.0200],\n",
       "        [  0.6183],\n",
       "        [  0.8913],\n",
       "        [  0.3577]], device='cuda:0'), loss=tensor([118.8360], device='cuda:0', grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
