{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b493f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import EcoPerceiverLoaderConfig, EcoPerceiverBatch, EcoPerceiverDataset\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import sqlite3\n",
    "from torch.utils.data import DataLoader\n",
    "from einops import rearrange\n",
    "from components import EcoSageConfig, ECInputModule, ModisLinearInputModule, AttentionLayer, FourierFeatureMapping, GeoInputModule, IGBPInputModule, PhenocamRGBInputModule\n",
    "from typing import Tuple, Dict, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33134f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = ['US-Whs', 'CA-Gro', 'DE-Geb', 'CN-Din']\n",
    "config = EcoPerceiverLoaderConfig(context_window_length=64, targets=['NEE'])\n",
    "ds = EcoPerceiverDataset('/data/fluxes/carbonsense_v2', config)\n",
    "dl = DataLoader(ds, batch_size=16, shuffle=True, collate_fn=ds.collate_fn, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10966622",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EcoSage(nn.Module):\n",
    "    def __init__(self, config: EcoSageConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.windowed_modules = nn.ModuleList([ECInputModule(config)])\n",
    "        self.auxiliary_modules = nn.ModuleList([ModisLinearInputModule(config), GeoInputModule(config), IGBPInputModule(config), PhenocamRGBInputModule(config)])\n",
    "\n",
    "        self.input_hidden_dim = 2 * self.config.num_frequencies + self.config.input_embedding_dim\n",
    "        self.latent_embeddings = nn.Embedding(self.config.context_length, self.config.latent_space_dim)\n",
    "        layers = []\n",
    "        for l in self.config.layers:\n",
    "            if l in ['w', 'c']:\n",
    "                layers.append(AttentionLayer(self.config.latent_space_dim, self.config.num_heads, self.config.mlp_ratio, kv_hidden_size=self.input_hidden_dim))\n",
    "            else:\n",
    "                layers.append(AttentionLayer(self.config.latent_space_dim, self.config.num_heads, self.config.mlp_ratio))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.output_proj = nn.Linear(self.config.latent_space_dim, 1)\n",
    "        self.apply(self.initialize_weights)\n",
    "    \n",
    "    def initialize_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_normal_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "        elif isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d, nn.LayerNorm)):\n",
    "            nn.init.constant_(module.weight, 1)\n",
    "            nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        windowed_inputs = []\n",
    "        windowed_masks = []\n",
    "        for m in self.windowed_modules:\n",
    "            ip, mask = m(batch)\n",
    "            windowed_inputs.append(ip)\n",
    "            windowed_masks.append(mask)\n",
    "        windowed_input = torch.cat(windowed_inputs, dim=-2)\n",
    "        windowed_mask = torch.cat(windowed_masks, dim=-1)\n",
    "\n",
    "        aux_inputs = []\n",
    "        aux_masks = []\n",
    "        for m in self.auxiliary_modules:\n",
    "            ip, mask = m(batch)\n",
    "            if ip == None or mask == None:\n",
    "                continue\n",
    "            aux_inputs.append(ip)\n",
    "            aux_masks.append(mask)\n",
    "        aux_input = torch.cat(aux_inputs, dim=-2)\n",
    "        aux_mask = torch.cat(aux_masks, dim=-1)\n",
    "\n",
    "        # print(windowed_input.shape)\n",
    "        # print(aux_input.shape)\n",
    "        # print()\n",
    "\n",
    "        B, L, _ = batch.predictor_values.shape\n",
    "        hidden = self.latent_embeddings.weight.unsqueeze(0).repeat(B,1,1)\n",
    "\n",
    "        for i, layer_type in enumerate(self.config.layers):\n",
    "            if layer_type == 'w':\n",
    "                hidden = rearrange(hidden, 'B L H -> (B L) H').unsqueeze(1)\n",
    "                hidden, _ = self.layers[i](hidden, windowed_input, mask=windowed_mask)\n",
    "                hidden = rearrange(hidden.squeeze(), '(B L) H -> B L H', B=B, L=L)\n",
    "            elif layer_type == 'c':\n",
    "                hidden, _ = self.layers[i](hidden, aux_input, mask=aux_mask)\n",
    "            else:\n",
    "                hidden, _ = self.layers[i](hidden)\n",
    "        \n",
    "        output = self.output_proj(hidden[:,-1,:]).squeeze()\n",
    "        return output\n",
    "\n",
    "# note: call Robert at 905 706 8876 (CRA)\n",
    "\n",
    "config = EcoSageConfig()\n",
    "model = EcoSage(config)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "print('Model created successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11ca5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in tqdm(dl):\n",
    "    model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0606a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with sqlite3.connect('/data/fluxes/carbonsense_v2/carbonsense_v2.sql') as conn:\n",
    "#     res =conn.execute('SELECT DISTINCT(igbp) FROM site_data;').fetchall()\n",
    "# [r[0] for r in res]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
